#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–æ–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
"""

import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent / "src"))

from src.document_processor import DocumentProcessor


def main():
    """–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —á—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∏–Ω–¥–µ–∫—Å–µ"""
    print("üîç –ß—Ç–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –≤–∞—à–µ–º RAG –∏–Ω–¥–µ–∫—Å–µ")
    print("=" * 50)

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    processor = DocumentProcessor()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
    loaded = processor.faiss_manager.load_index()

    if not loaded:
        print("‚ùå –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python test_your_documents.py")
        return

    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = processor.get_index_statistics()

    print(f"üìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   üìÑ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats.get('sources_count', 0)}")
    print(f"   üß© –í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (—á–∞–Ω–∫–æ–≤): {stats.get('total_chunks', 0)}")
    print(f"   üî¢ –í–µ–∫—Ç–æ—Ä–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ: {stats.get('total_vectors', 0)}")
    print(f"   üìù –í—Å–µ–≥–æ —Å–∏–º–≤–æ–ª–æ–≤: {stats.get('total_characters', 0):,}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–∫–∏–µ —Ñ–∞–π–ª—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
    if 'sources_distribution' in stats:
        print(f"\nüìÅ –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        for i, (filename, chunks_count) in enumerate(stats['sources_distribution'].items(), 1):
            print(f"   {i}. {filename}")
            print(f"      –†–∞–∑–±–∏—Ç–æ –Ω–∞ {chunks_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    if 'categories_distribution' in stats:
        print(f"\nüìÇ –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for category, count in stats['categories_distribution'].items():
            print(f"   {category}: {count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    print(f"\nüîç –¢–µ—Å—Ç –ø–æ–∏—Å–∫–∞:")
    test_queries = ["—ç–ª–µ–∫—Ç—Ä–∏–∫–∞", "–∞–ª—å–±–æ–º", "–ø—Ä–æ–µ–∫—Ç"]

    for query in test_queries:
        results = processor.search_documents(query, k=2)
        print(f"\n   –ü–æ–∏—Å–∫ '{query}':")
        if results:
            for j, result in enumerate(results, 1):
                print(f"     {j}. {result['source_file']} (—Ç–æ—á–Ω–æ—Å—Ç—å: {result['score']:.2f})")
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                preview = result['text'].replace('\n', ' ')[:100] + "..."
                print(f"        ¬´{preview}¬ª")
        else:
            print(f"     –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä —á–∞–Ω–∫–∞
    print(f"\nüìñ –ü—Ä–∏–º–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞:")
    all_chunks = processor.faiss_manager.get_all_chunks()
    if all_chunks:
        example_chunk = all_chunks[0]
        print(f"   –§–∞–π–ª: {example_chunk['source_file']}")
        print(f"   –†–∞–∑–º–µ—Ä: {len(example_chunk['text'])} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ù–∞—á–∞–ª–æ —Ç–µ–∫—Å—Ç–∞:")
        preview_text = example_chunk['text'][:300].replace('\n', '\n   ')
        print(f"   ¬´{preview_text}...¬ª")

    print(f"\n‚úÖ –ò–Ω–¥–µ–∫—Å —Ä–∞–±–æ—Ç–∞–µ—Ç! –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ø–æ–∏—Å–∫–∞.")
    print(f"üí° –î–ª—è –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: processor.search_documents('–≤–∞—à –∑–∞–ø—Ä–æ—Å')")


if __name__ == "__main__":
    main()