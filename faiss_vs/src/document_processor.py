import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
import logging
import numpy as np

from .data.loaders import DocumentLoader, DocumentParser
from .data.chunkers import DocumentChunker, TextChunk
from .data.image_processor import ImageProcessor, MultiModalProcessor
from .vectorstore.faiss_manager import FAISSManager, create_faiss_manager
from .config import settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DocumentProcessor:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è RAG –ø–∞–π–ø–ª–∞–π–Ω–æ–º —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏"""

    def __init__(self, client_id: str, enable_visual_search: bool = None):  # ‚úÖ –î–û–ë–ê–í–õ–ï–ù –ø–∞—Ä–∞–º–µ—Ç—Ä
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        Args:
            client_id: ID –∫–ª–∏–µ–Ω—Ç–∞
            enable_visual_search: –í–∫–ª—é—á–∏—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫. None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        """
        self.client_id = client_id

        # ‚úÖ –ù–û–í–ê–Ø –ª–æ–≥–∏–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–∂–∏–º–∞
        if enable_visual_search is None:
            self.enable_visual_search = settings.ENABLE_VISUAL_SEARCH_BY_DEFAULT
        else:
            self.enable_visual_search = enable_visual_search

        logger.info(
            f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DocumentProcessor –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞ {client_id}, –≤–∏–∑—É–∞–ª—å–Ω—ã–π_–ø–æ–∏—Å–∫={self.enable_visual_search}")

        # ‚úÖ –û–ë–ù–û–í–õ–ï–ù–ù–ê–Ø –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.document_loader = DocumentLoader(client_id=client_id)  # –ü–µ—Ä–µ–¥–∞–µ–º client_id
        self.document_parser = DocumentParser()
        self.chunker = DocumentChunker()

        # ‚úÖ –ù–û–í–´–ï –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if self.enable_visual_search:
            try:
                self.multimodal_processor = MultiModalProcessor()
                logger.info("üñºÔ∏è –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º –≤–∫–ª—é—á–µ–Ω - –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä: {e}")
                logger.info("üìù –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º")
                self.enable_visual_search = False
                self.image_processor = ImageProcessor()
        else:
            self.image_processor = ImageProcessor()
            logger.info("üìù –¢–µ–∫—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º - —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")

        # ‚úÖ –û–ë–ù–û–í–õ–ï–ù–ù–´–ô FAISS –º–µ–Ω–µ–¥–∂–µ—Ä
        self.faiss_manager = create_faiss_manager(client_id, self.enable_visual_search)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self.faiss_manager.load_index()

    def process_documents_from_json(self, json_file_path: str,
                                    update_existing: bool = False) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ JSON —Ñ–∞–π–ª–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏

        Args:
            json_file_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            update_existing: –û–±–Ω–æ–≤–ª—è—Ç—å –ª–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        logger.info(
            f"–ù–∞—á–∏–Ω–∞–µ–º {'–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é' if self.enable_visual_search else '—Ç–µ–∫—Å—Ç–æ–≤—É—é'} –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ {json_file_path}")

        # ‚úÖ –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            'total_documents': 0,
            'downloaded': 0,
            'parsed': 0,
            'chunked': 0,
            'indexed': 0,
            'images_processed': 0,  # ‚úÖ –ù–û–í–û–ï
            'visual_vectors_created': 0,  # ‚úÖ –ù–û–í–û–ï
            'text_vectors_created': 0,  # ‚úÖ –ù–û–í–û–ï
            'errors': [],
            'processed_files': [],
            'start_time': datetime.now().isoformat(),
            'multimodal_mode': self.enable_visual_search  # ‚úÖ –ù–û–í–û–ï
        }

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ —Å–∫–∞—á–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            documents_info = self.document_loader.process_json_documents(json_file_path)
            stats['total_documents'] = len(documents_info)

            all_chunks = []

            for doc_info in documents_info:
                try:
                    file_path = doc_info['file_path']
                    url = doc_info['url']
                    original_metadata = doc_info['metadata']

                    logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º: {file_path.name}")

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª—è—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —á–∞–Ω–∫–∏
                    if not update_existing:
                        existing_chunks = self.faiss_manager.get_chunks_by_source(file_path.name)
                        if existing_chunks:
                            logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç {file_path.name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                            continue

                    stats['downloaded'] += 1

                    # ‚úÖ –ù–û–í–ê–Ø –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
                    is_image = self._is_image_file(file_path)

                    if is_image:
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        text, visual_vector, enhanced_metadata = self._process_image(
                            file_path, url, original_metadata
                        )
                        stats['images_processed'] += 1

                        if self.enable_visual_search and visual_vector is not None:
                            stats['visual_vectors_created'] += 1

                    else:
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
                        text, doc_metadata = self.document_parser.parse_document(file_path)
                        visual_vector = None

                        enhanced_metadata = self._create_enhanced_metadata(
                            original_metadata, url, file_path, doc_metadata
                        )

                    if not text.strip():
                        stats['errors'].append(f"–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –≤ {file_path.name}")
                        continue

                    stats['parsed'] += 1

                    # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
                    chunks = self.chunker.create_chunks(text, file_path.name, enhanced_metadata)

                    if chunks:
                        # ‚úÖ –ù–û–í–ê–Ø –ª–æ–≥–∏–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤
                        for chunk in chunks:
                            if visual_vector is not None and self.enable_visual_search:
                                # –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π —á–∞–Ω–∫
                                self.faiss_manager.add_multimodal_chunk(chunk, visual_vector)
                            else:
                                # –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π —á–∞–Ω–∫ (—Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞)
                                if self.enable_visual_search:
                                    self.faiss_manager.add_text_chunk(chunk)
                                else:
                                    # –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º API
                                    pass  # –ë—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω —á–µ—Ä–µ–∑ add_chunks –Ω–∏–∂–µ

                        if not self.enable_visual_search:
                            # –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –ø–æ–ª–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                            self.faiss_manager.add_chunks(chunks)

                        all_chunks.extend(chunks)
                        stats['chunked'] += len(chunks)
                        stats['text_vectors_created'] += len(chunks)

                        stats['processed_files'].append({
                            'filename': file_path.name,
                            'file_type': 'image' if is_image else 'document',
                            'chunks_count': len(chunks),
                            'characters': len(text),
                            'has_visual_vector': visual_vector is not None,
                            'url': url,
                            'metadata_keys': list(enhanced_metadata.keys())
                        })

                        logger.info(
                            f"‚úÖ {file_path.name}: {len(chunks)} —á–∞–Ω–∫–æ–≤, –≤–∏–∑—É–∞–ª—å–Ω—ã–π={'–¥–∞' if visual_vector is not None else '–Ω–µ—Ç'}")

                    # ‚úÖ –ù–û–í–û–ï: –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    if not settings.KEEP_DOWNLOADED_FILES:
                        try:
                            file_path.unlink()
                            logger.debug(f"üóëÔ∏è –§–∞–π–ª {file_path.name} —É–¥–∞–ª–µ–Ω –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å —Ñ–∞–π–ª {file_path.name}: {e}")

                except Exception as e:
                    error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {doc_info.get('file_path', 'unknown')}: {e}"
                    logger.error(error_msg)
                    stats['errors'].append(error_msg)
                    continue

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
            if all_chunks:
                self.faiss_manager.save_index()
                stats['indexed'] = len(all_chunks)
                logger.info(
                    f"‚úÖ –ò–Ω–¥–µ–∫—Å—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {stats['text_vectors_created']} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö, {stats['visual_vectors_created']} –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö")

            stats['end_time'] = datetime.now().isoformat()
            stats['success'] = True

            # ‚úÖ –ù–û–í–û–ï: –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ CLIP
            if self.enable_visual_search and hasattr(self, 'multimodal_processor'):
                self.multimodal_processor.cleanup_gpu_memory()

        except Exception as e:
            stats['end_time'] = datetime.now().isoformat()
            stats['success'] = False
            stats['errors'].append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")

        return stats

    # ‚úÖ –ù–û–í–´–ï –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã

    def _is_image_file(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º"""
        image_extensions = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp'}
        return file_path.suffix.lower() in image_extensions

    def _process_image(self, file_path: Path, url: str, original_metadata: Dict) -> tuple[
        str, Optional[np.ndarray], Dict]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞"""

        if self.enable_visual_search and hasattr(self, 'multimodal_processor'):
            # –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            text, visual_vector, updated_metadata = self.multimodal_processor.process_image_multimodal(
                file_path, original_metadata
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            enhanced_metadata = self._create_enhanced_metadata(
                updated_metadata, url, file_path, None
            )

            return text, visual_vector, enhanced_metadata

        else:
            # –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
            text = self.image_processor.create_image_embedding_text(file_path, original_metadata)

            enhanced_metadata = self._create_enhanced_metadata(
                original_metadata, url, file_path, None
            )

            return text, None, enhanced_metadata

    # –í document_processor.py –≤ —Ñ—É–Ω–∫—Ü–∏–∏ _create_enhanced_metadata –ó–ê–ú–ï–ù–ò–¢–ï:

    def _create_enhanced_metadata(self, original_metadata: Dict, url: str,
                                  file_path: Path, doc_metadata: Any) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""

        enhanced_metadata = {}

        # –°–Ω–∞—á–∞–ª–∞ –∫–æ–ø–∏—Ä—É–µ–º –í–°–Å –∏–∑ original_metadata
        enhanced_metadata.update(original_metadata)

        # –ü–æ—Ç–æ–º –¥–æ–ø–æ–ª–Ω—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –ø–æ–ª—è–º–∏
        enhanced_metadata.update({
            'file_type': file_path.suffix.lower(),
            'filename': file_path.name,
            'file_size': file_path.stat().st_size if file_path.exists() else 0,
            'processing_date': datetime.now().isoformat(),
            'client_id': self.client_id,
        })

        # –î–æ–ø–æ–ª–Ω—è–µ–º –∏–∑ doc_metadata, –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ enhanced_metadata –Ω–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è –∏–ª–∏ –æ–Ω–æ –ø—É—Å—Ç–æ–µ
        if doc_metadata:
            for key, value in doc_metadata.__dict__.items():
                if not enhanced_metadata.get(key) and value not in (None, ''):
                    enhanced_metadata[key] = value

        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –≤–∞–∂–Ω—ã–µ –ø–æ–ª—è –≤—Å–µ–≥–¥–∞ –µ—Å—Ç—å
        if not enhanced_metadata.get('source_url'):
            enhanced_metadata['source_url'] = url

        if not enhanced_metadata.get('category'):
            enhanced_metadata['category'] = enhanced_metadata.get('parent', 'uncategorized')

        if not enhanced_metadata.get('title'):
            enhanced_metadata['title'] = enhanced_metadata.get('description', file_path.stem)

        # –û—Ç–ª–∞–¥–∫–∞
        logger.info(f"üîß DEBUG enhanced_metadata —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è {file_path.name}:")
        logger.info(f"   enhanced_metadata['source_url']: '{enhanced_metadata.get('source_url')}'")
        logger.info(f"   enhanced_metadata['description']: '{enhanced_metadata.get('description')}'")
        logger.info(f"   enhanced_metadata['title']: '{enhanced_metadata.get('title')}'")
        logger.info(f"   enhanced_metadata keys count: {len(enhanced_metadata)}")

        return enhanced_metadata

    def search_documents(self, query: str = None, k: int = 5,
                         min_score: float = 0.0,
                         filters: Optional[Dict[str, Any]] = None,
                         search_mode: str = "auto") -> List[Dict[str, Any]]:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏

        Args:
            query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_score: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            filters: –§–∏–ª—å—Ç—Ä—ã –ø–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, {'category': 'tech'})
            search_mode: –†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞ ("auto", "text", "visual_description")

        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
        """
        if not query:
            return []

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞
        if search_mode == "auto":
            if self.enable_visual_search:
                # –í –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É
                search_mode = "text"
            else:
                search_mode = "text"

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        if search_mode == "text":
            results = self.faiss_manager.search(query, k=k * 2, score_threshold=min_score)
        elif search_mode == "visual_description" and self.enable_visual_search:
            # –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é —á–µ—Ä–µ–∑ CLIP
            results = self.search_by_text_description(query, k=k * 2, search_images_only=False)
        else:
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
            results = self.faiss_manager.search(query, k=k * 2, score_threshold=min_score)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
        if filters:
            results = self._apply_filters(results, filters)

        return results[:k]

    def search_similar_images(self, query_image_path: Union[str, Path], k: int = 5,
                              min_score: float = 0.0) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –≤–∏–∑—É–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

        Args:
            query_image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é-–∑–∞–ø—Ä–æ—Å—É
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_score: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π score —Å—Ö–æ–¥—Å—Ç–≤–∞

        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å scores
        """
        if not self.enable_visual_search:
            logger.warning("–í–∏–∑—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –æ—Ç–∫–ª—é—á–µ–Ω. –í–∫–ª—é—á–∏—Ç–µ enable_visual_search=True")
            return []

        if not hasattr(self, 'multimodal_processor'):
            logger.error("–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return []

        try:
            # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            query_path = Path(query_image_path)
            visual_query = self.multimodal_processor.create_visual_embedding(query_path)

            # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
            results = self.faiss_manager.search_visual(visual_query, k=k, score_threshold=min_score)

            logger.info(f"üîç –í–∏–∑—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ {query_path.name}: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return results

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def search_by_text_description(self, text_description: str, k: int = 5,
                                   search_images_only: bool = True) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é —á–µ—Ä–µ–∑ CLIP

        Args:
            text_description: –û–ø–∏—Å–∞–Ω–∏–µ –∏—Å–∫–æ–º–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ("—Ñ–∞—Å–∞–¥ –∑–¥–∞–Ω–∏—è")
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            search_images_only: –ò—Å–∫–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å—Ä–µ–¥–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

        Returns:
            List[Dict]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        """
        if not self.enable_visual_search:
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
            return self.search_documents(text_description, k=k)

        if not hasattr(self, 'multimodal_processor'):
            logger.error("–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return self.search_documents(text_description, k=k)

        try:
            # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∏–∑ —Ç–µ–∫—Å—Ç–∞
            visual_query = self.multimodal_processor.search_by_text_description(text_description)

            # –ò—â–µ–º –≤ –≤–∏–∑—É–∞–ª—å–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
            results = self.faiss_manager.search_visual(visual_query, k=k * 2)

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if search_images_only:
                image_results = []
                for result in results:
                    metadata = result.get('metadata', {})
                    if metadata.get('is_image', False):
                        image_results.append(result)
                results = image_results[:k]

            logger.info(f"üéØ –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é '{text_description}': –Ω–∞–π–¥–µ–Ω–æ {len(results)}")
            return results

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é: {e}")
            return []

    def search_multimodal(self, text_query: str = None, image_query_path: Union[str, Path] = None,
                          k: int = 5, text_weight: float = 0.6) -> List[Dict[str, Any]]:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫

        Args:
            text_query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            image_query_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é-–∑–∞–ø—Ä–æ—Å—É
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            text_weight: –í–µ—Å —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞ (0.0-1.0)

        Returns:
            List[Dict]: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        """
        if not self.enable_visual_search:
            logger.warning("–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –±–µ–∑ visual_search")
            return self.search_documents(text_query, k=k) if text_query else []

        if not hasattr(self, 'multimodal_processor'):
            logger.error("–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return self.search_documents(text_query, k=k) if text_query else []

        try:
            visual_query = None
            if image_query_path:
                visual_query = self.multimodal_processor.create_visual_embedding(Path(image_query_path))

            results = self.faiss_manager.search_multimodal(
                text_query=text_query,
                visual_query=visual_query,
                k=k,
                text_weight=text_weight
            )

            logger.info(
                f"üîç –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫: —Ç–µ–∫—Å—Ç='{text_query}', –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ={'–¥–∞' if image_query_path else '–Ω–µ—Ç'}, –Ω–∞–π–¥–µ–Ω–æ={len(results)}")
            return results

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []

    def get_image_analysis(self, image_path: Union[str, Path]) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é CLIP

        Args:
            image_path: –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é

        Returns:
            Dict: –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        if not self.enable_visual_search or not hasattr(self, 'multimodal_processor'):
            return {'error': '–í–∏–∑—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}

        try:
            image_path = Path(image_path)

            # –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
            visual_vector = self.multimodal_processor.create_visual_embedding(image_path)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            categories = self.multimodal_processor.get_image_categories(image_path)

            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            top_categories = dict(list(categories.items())[:3])

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            building_categories = ["—Ñ–∞—Å–∞–¥ –∑–¥–∞–Ω–∏—è", "–≤–Ω–µ—à–Ω–∏–π –≤–∏–¥ –∑–¥–∞–Ω–∏—è", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞"]
            document_categories = ["–¥–æ–∫—É–º–µ–Ω—Ç", "—Ç–µ–∫—Å—Ç –Ω–∞ –±—É–º–∞–≥–µ", "—á–µ—Ä—Ç–µ–∂", "—Å—Ö–µ–º–∞"]

            building_scores = [categories.get(cat, 0) for cat in building_categories]
            document_scores = [categories.get(cat, 0) for cat in document_categories]

            max_building = max(building_scores) if building_scores else 0
            max_document = max(document_scores) if document_scores else 0

            main_type = "building" if max_building > max_document else "document"

            return {
                'image_path': str(image_path),
                'visual_vector_shape': visual_vector.shape,
                'main_content_type': main_type,
                'building_confidence': max_building,
                'document_confidence': max_document,
                'top_categories': top_categories,
                'all_categories': categories,
                'analysis_successful': True
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path}: {e}")
            return {'error': str(e), 'analysis_successful': False}

    def find_images_by_category(self, category: str, k: int = 10) -> List[Dict[str, Any]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏

        Args:
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ ("—Ñ–∞—Å–∞–¥", "–¥–æ–∫—É–º–µ–Ω—Ç", etc.)
            k: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            List[Dict]: –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –æ–ø–∏—Å–∞–Ω–∏—é —á–µ—Ä–µ–∑ –≤–∏–∑—É–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ
        if self.enable_visual_search:
            return self.search_by_text_description(category, k=k, search_images_only=True)
        else:
            # Fallback –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º
            results = self.search_documents(category, k=k * 2)
            image_results = []
            for result in results:
                metadata = result.get('metadata', {})
                if metadata.get('is_image', False):
                    image_results.append(result)
            return image_results[:k]

    def get_similar_to_existing(self, source_file: str, k: int = 5) -> List[Dict[str, Any]]:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–µ –≤ –∏–Ω–¥–µ–∫—Å–µ

        Args:
            source_file: –ò–º—è —Ñ–∞–π–ª–∞ –≤ –∏–Ω–¥–µ–∫—Å–µ
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ö–æ–∂–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

        Returns:
            List[Dict]: –ü–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        if not self.enable_visual_search:
            return []

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π –º–µ—Ç–æ–¥ FAISS –º–µ–Ω–µ–¥–∂–µ—Ä–∞
            return self.faiss_manager.get_similar_visual_chunks(source_file, k=k)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –Ω–∞ {source_file}: {e}")
            return []

    def _apply_filters(self, results: List[Dict[str, Any]], filters: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –ø–æ–∏—Å–∫–∞"""
        filtered_results = []

        for result in results:
            metadata = result.get('metadata', {})
            should_include = True

            for filter_key, filter_value in filters.items():
                if filter_key not in metadata:
                    should_include = False
                    break

                if isinstance(filter_value, list):
                    if metadata[filter_key] not in filter_value:
                        should_include = False
                        break
                else:
                    if metadata[filter_key] != filter_value:
                        should_include = False
                        break

            if should_include:
                filtered_results.append(result)

        return filtered_results

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–Ω–µ –∏–∑–º–µ–Ω–µ–Ω—ã)

    def get_document_by_source(self, source_file: str) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        return self.faiss_manager.get_chunks_by_source(source_file)

    def remove_document(self, source_file: str) -> bool:
        """–£–¥–∞–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç –∏–∑ –∏–Ω–¥–µ–∫—Å–∞"""
        chunks = self.faiss_manager.get_chunks_by_source(source_file)
        if chunks:
            chunk_ids = [chunk.get('chunk_id') for chunk in chunks if chunk.get('chunk_id')]
            success = self.faiss_manager.remove_chunks(chunk_ids)
            if success:
                self.faiss_manager.save_index()
                logger.info(f"–î–æ–∫—É–º–µ–Ω—Ç {source_file} —É–¥–∞–ª–µ–Ω –∏–∑ –∏–Ω–¥–µ–∫—Å–∞")
            return success
        return False

    def update_document(self, json_file_path: str, source_file: str) -> Dict[str, Any]:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç"""
        # –°–Ω–∞—á–∞–ª–∞ —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é
        self.remove_document(source_file)

        # –ó–∞—Ç–µ–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–Ω–æ–≤–æ
        return self.process_documents_from_json(json_file_path, update_existing=True)

    def get_index_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–Ω–¥–µ–∫—Å–∞"""
        base_stats = self.faiss_manager.get_index_statistics()

        # ‚úÖ –ù–û–í–ê–Ø —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        if base_stats.get('status') == 'ready':
            all_chunks = self.faiss_manager.get_all_chunks()

            # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            images_count = 0
            documents_count = 0
            multimodal_count = 0
            categories = {}
            file_types = {}
            total_chars = 0

            for chunk in all_chunks:
                metadata = chunk.get('metadata', {})

                # –ü–æ–¥—Å—á–µ—Ç —Ç–∏–ø–æ–≤
                if metadata.get('is_image', False):
                    images_count += 1
                else:
                    documents_count += 1

                if chunk.get('has_visual_vector', False):
                    multimodal_count += 1

                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
                category = metadata.get('category', 'uncategorized')
                categories[category] = categories.get(category, 0) + 1

                # –¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤
                file_type = metadata.get('file_type', 'unknown')
                file_types[file_type] = file_types.get(file_type, 0) + 1

                # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤
                total_chars += len(chunk.get('text', ''))

            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            base_stats.update({
                'content_analysis': {
                    'images_count': images_count,
                    'documents_count': documents_count,
                    'multimodal_count': multimodal_count,
                    'visual_coverage': multimodal_count / len(all_chunks) if all_chunks else 0
                },
                'file_types_distribution': file_types,
                'categories_distribution': categories,
                'total_characters': total_chars,
                'average_chunk_size': total_chars / len(all_chunks) if all_chunks else 0,
                'search_capabilities': {
                    'text_search': True,
                    'visual_search': self.enable_visual_search,
                    'multimodal_search': self.enable_visual_search,
                    'similar_images': self.enable_visual_search,
                    'image_analysis': self.enable_visual_search
                }
            })

        return base_stats

    def clear_all_data(self, client_id: Optional[str] = None):
        """–û—á–∏—â–∞–µ—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ!)"""
        target_client = client_id or self.client_id
        logger.warning(f"–û—á–∏—â–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ –∫–ª–∏–µ–Ω—Ç–∞ {target_client}")
        self.faiss_manager.clear_index()

    def export_chunks_to_json(self, output_path: str):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ –≤ JSON —Ñ–∞–π–ª"""
        all_chunks = self.faiss_manager.get_all_chunks()

        export_data = {
            'exported_at': datetime.now().isoformat(),
            'client_id': self.client_id,
            'enable_visual_search': self.enable_visual_search,
            'total_chunks': len(all_chunks),
            'chunks': all_chunks
        }

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)

        logger.info(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤ –≤ {output_path}")

    # ‚úÖ –ù–û–í–´–ï –º–µ—Ç–æ–¥—ã –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞

    def get_processing_mode_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–∂–∏–º–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        mode_info = {
            'client_id': self.client_id,
            'visual_search_enabled': self.enable_visual_search,
            'capabilities': {
                'text_search': True,
                'image_search_by_description': self.enable_visual_search,
                'similar_image_search': self.enable_visual_search,
                'multimodal_combined_search': self.enable_visual_search,
                'image_analysis': self.enable_visual_search
            }
        }

        if self.enable_visual_search and hasattr(self, 'multimodal_processor'):
            mode_info['multimodal_model_info'] = self.multimodal_processor.get_model_info()

        return mode_info

    def export_visual_vectors(self, output_path: str):
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        if not self.enable_visual_search:
            logger.warning("–ù–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
            return

        try:
            export_data = self.faiss_manager.export_visual_vectors()

            if 'error' in export_data:
                logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞: {export_data['error']}")
                return

            export_data['exported_at'] = datetime.now().isoformat()

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)

            logger.info(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {export_data['visual_vectors_count']} –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ –≤ {output_path}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤: {e}")


# ‚úÖ –ù–û–í–´–ï —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–≤

def create_text_processor(client_id: str) -> DocumentProcessor:
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    return DocumentProcessor(client_id=client_id, enable_visual_search=False)


def create_multimodal_processor(client_id: str) -> DocumentProcessor:
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    return DocumentProcessor(client_id=client_id, enable_visual_search=True)


def create_auto_processor(client_id: str) -> DocumentProcessor:
    """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ–∂–∏–º–∞ –ø–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    return DocumentProcessor(client_id=client_id, enable_visual_search=None)


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ DocumentProcessor"""

    print("üöÄ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ DocumentProcessor")
    print("=" * 70)

    client_id = "demo_client"

    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä (–∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞)
    processor = create_auto_processor(client_id)

    print("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–µ:")
    mode_info = processor.get_processing_mode_info()
    for key, value in mode_info.items():
        if key != 'multimodal_model_info':
            print(f"   {key}: {value}")

    print(f"\nüîç –î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ–∏—Å–∫–∞:")
    capabilities = mode_info['capabilities']
    for capability, available in capabilities.items():
        status = "‚úÖ" if available else "‚ùå"
        print(f"   {status} {capability}")

    print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:")

    print(f"\n1. –û–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫:")
    print(f"   results = processor.search_documents('—Ñ–∞—Å–∞–¥ –∑–¥–∞–Ω–∏—è')")

    if processor.enable_visual_search:
        print(f"\n2. –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é:")
        print(f"   results = processor.search_by_text_description('—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞—Å–∞–¥')")

        print(f"\n3. –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:")
        print(f"   results = processor.search_similar_images('my_facade.jpg')")

        print(f"\n4. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫:")
        print(f"   results = processor.search_multimodal(")
        print(f"       text_query='—Ñ–∞—Å–∞–¥',")
        print(f"       image_query_path='example.jpg'")
        print(f"   )")

        print(f"\n5. –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:")
        print(f"   analysis = processor.get_image_analysis('facade.jpg')")

    print(f"\n6. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞:")
    print(f"   stats = processor.get_index_statistics()")

    print(f"\n‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π DocumentProcessor –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")


if __name__ == "__main__":
    main()