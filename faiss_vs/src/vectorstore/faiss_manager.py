import faiss
import numpy as np
import pickle
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import logging
from datetime import datetime

from sentence_transformers import SentenceTransformer
from ..data.chunkers import TextChunk
from ..config import settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FAISSManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å FAISS –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏"""

    def __init__(self,
                 client_id: str,
                 model_name: str = settings.EMBEDDING_MODEL,
                 index_type: str = settings.FAISS_INDEX_TYPE,
                 enable_visual_search: bool = False):  # ‚úÖ –î–û–ë–ê–í–õ–ï–ù –ø–∞—Ä–∞–º–µ—Ç—Ä

        self.client_id = client_id
        self.model_name = model_name
        self.index_type = index_type
        self.enable_visual_search = enable_visual_search  # ‚úÖ –ù–û–í–û–ï

        # –ú–æ–¥–µ–ª–∏ –∏ –∏–Ω–¥–µ–∫—Å—ã
        self.embedding_model = None
        self.index = None  # –°—Ç–∞—Ä—ã–π –µ–¥–∏–Ω—ã–π –∏–Ω–¥–µ–∫—Å (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        self.text_index = None  # ‚úÖ –ù–û–í–û–ï: –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤
        self.visual_index = None  # ‚úÖ –ù–û–í–û–ï: –î–ª—è –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤

        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –º–∞–ø–ø–∏–Ω–≥–∏
        self.metadata = {}
        self.id_to_chunk_id = {}
        self.chunk_id_to_id = {}
        # ‚úÖ –ù–û–í–´–ï –º–∞–ø–ø–∏–Ω–≥–∏ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        self.text_id_to_chunk_id = {}
        self.visual_id_to_chunk_id = {}
        self.chunk_id_to_ids = {}  # {chunk_id: {'text_id': X, 'visual_id': Y}}

        # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.dimension = settings.EMBEDDING_DIMENSION
        self.text_dimension = settings.EMBEDDING_DIMENSION
        self.visual_dimension = settings.VISUAL_EMBEDDING_DIMENSION  # ‚úÖ –ù–û–í–û–ï

        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫: faiss_index/clients/client_id
        self.client_dir = settings.CLIENTS_DIR / client_id
        self.client_dir.mkdir(parents=True, exist_ok=True)

        # –ü—É—Ç–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        self.index_path = self.client_dir / "index.faiss"  # –°—Ç–∞—Ä—ã–π –ø—É—Ç—å
        self.text_index_path = self.client_dir / "text_index.faiss"  # ‚úÖ –ù–û–í–û–ï
        self.visual_index_path = self.client_dir / "visual_index.faiss"  # ‚úÖ –ù–û–í–û–ï
        self.metadata_path = self.client_dir / "metadata.pkl"
        self.mappings_path = self.client_dir / "mappings.json"
        self.config_path = self.client_dir / "config.json"

    def initialize_embedding_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings"""
        if self.embedding_model is None:
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å embeddings: {self.model_name}")
            self.embedding_model = SentenceTransformer(self.model_name)
            self.text_dimension = self.embedding_model.get_sentence_embedding_dimension()

    def create_index(self, force_recreate: bool = False):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π FAISS –∏–Ω–¥–µ–∫—Å"""
        if not force_recreate:
            if not self.enable_visual_search and self.index is not None:
                logger.warning("–ò–Ω–¥–µ–∫—Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ force_recreate=True –¥–ª—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è")
                return
            elif self.enable_visual_search and (self.text_index is not None or self.visual_index is not None):
                logger.warning("–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ force_recreate=True")
                return

        logger.info(f"–°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å(—ã) —Ç–∏–ø–∞ {self.index_type}")

        if self.enable_visual_search:
            # ‚úÖ –ú–£–õ–¨–¢–ò–ú–û–î–ê–õ–¨–ù–´–ô —Ä–µ–∂–∏–º: —Å–æ–∑–¥–∞–µ–º –¥–≤–∞ –∏–Ω–¥–µ–∫—Å–∞
            logger.info("–°–æ–∑–¥–∞–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã")

            if self.index_type == "FlatIP":
                self.text_index = faiss.IndexFlatIP(self.text_dimension)
                self.visual_index = faiss.IndexFlatIP(self.visual_dimension)
            elif self.index_type == "FlatL2":
                self.text_index = faiss.IndexFlatL2(self.text_dimension)
                self.visual_index = faiss.IndexFlatL2(self.visual_dimension)
            elif self.index_type == "HNSW":
                self.text_index = faiss.IndexHNSWFlat(self.text_dimension, 32)
                self.visual_index = faiss.IndexHNSWFlat(self.visual_dimension, 32)
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏–Ω–¥–µ–∫—Å–∞: {self.index_type}")

            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å: {self.text_dimension}D")
            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –≤–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {self.visual_dimension}D")

            # –û—á–∏—â–∞–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.text_id_to_chunk_id = {}
            self.visual_id_to_chunk_id = {}
            self.chunk_id_to_ids = {}

        else:
            # –û–ë–´–ß–ù–´–ô —Ä–µ–∂–∏–º: —Å–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—ã–π –∏–Ω–¥–µ–∫—Å (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)
            if self.index_type == "FlatIP":
                self.index = faiss.IndexFlatIP(self.dimension)
            elif self.index_type == "FlatL2":
                self.index = faiss.IndexFlatL2(self.dimension)
            elif self.index_type == "HNSW":
                self.index = faiss.IndexHNSWFlat(self.dimension, 32)
                self.index.hnsw.efConstruction = 200
                self.index.hnsw.efSearch = 128
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –∏–Ω–¥–µ–∫—Å–∞: {self.index_type}")

            logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω –µ–¥–∏–Ω—ã–π –∏–Ω–¥–µ–∫—Å: {self.dimension}D")

            # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.id_to_chunk_id = {}
            self.chunk_id_to_id = {}

        # –û—á–∏—â–∞–µ–º –æ–±—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.metadata = {}

    def create_embeddings(self, texts: List[str]) -> np.ndarray:
        """–°–æ–∑–¥–∞–µ—Ç embeddings –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        self.initialize_embedding_model()

        logger.info(f"–°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–ª—è cosine similarity (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º Inner Product)
        if self.index_type == "FlatIP":
            embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

        return embeddings.astype(np.float32)

    def add_chunks(self, chunks: List[TextChunk]) -> List[int]:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫–∏ –≤ –∏–Ω–¥–µ–∫—Å"""
        if self.index is None:
            self.create_index()

        if not chunks:
            return []

        # üîß –ò–°–ü–†–ê–í–õ–Ø–ï–ú: –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –í–ö–õ–Æ–ß–ê–Ø –ú–ï–¢–ê–î–ê–ù–ù–´–ï
        texts_for_embedding = []

        for chunk in chunks:
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            full_text_parts = [chunk.text]  # –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞

            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ —Ç–µ–∫—Å—Ç—É –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
            metadata = chunk.metadata

            if metadata.get('title'):
                full_text_parts.append(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {metadata['title']}")

            if metadata.get('description'):
                full_text_parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {metadata['description']}")

            if metadata.get('category'):
                full_text_parts.append(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {metadata['category']}")

            if metadata.get('parent'):
                full_text_parts.append(f"–†–∞–∑–¥–µ–ª: {metadata['parent']}")

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞—Å—Ç–∏
            combined_text = " ".join(full_text_parts)
            texts_for_embedding.append(combined_text)

            # üîß –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–´–ô –í–´–í–û–î –≤ stdout –ò –≤ logger
            debug_message = f"üîß –í–ï–ö–¢–û–†–ò–ó–ê–¶–ò–Ø {chunk.source_file} (—á–∞–Ω–∫ {chunk.chunk_index}):"
            # print(debug_message)  # –ü—Ä—è–º–æ–π –≤—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
            logger.info(debug_message)  # –ß–µ—Ä–µ–∑ logger
            #
            info_message = f"   üìù –ò—Å—Ö–æ–¥–Ω—ã–π: {len(chunk.text)} —Å–∏–º–≤–æ–ª–æ–≤ | –ò—Ç–æ–≥–æ–≤—ã–π: {len(combined_text)} —Å–∏–º–≤–æ–ª–æ–≤"
            # print(info_message)
            logger.info(info_message)
            #
            meta_message = f"   üè∑Ô∏è title='{metadata.get('title', '–ù–ï–¢')}' | description='{metadata.get('description', '–ù–ï–¢')}' | category='{metadata.get('category', '–ù–ï–¢')}'"
            # print(meta_message)
            logger.info(meta_message)

            if len(combined_text) > len(chunk.text) + 50:  # –ï—Å–ª–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–±–∞–≤–∏–ª–∏—Å—å
                added_meta = combined_text[len(chunk.text):100] + "..."
                added_message = f"   ‚ûï –î–æ–±–∞–≤–ª–µ–Ω–æ –∫ —Ç–µ–∫—Å—Ç—É: '{added_meta}'"
                # print(added_message)
                logger.info(added_message)
            else:
                print("   ‚ö†Ô∏è –ú–ï–¢–ê–î–ê–ù–ù–´–ï –ù–ï –î–û–ë–ê–í–ò–õ–ò–°–¨ –ö –¢–ï–ö–°–¢–£!")
                logger.warning("   ‚ö†Ô∏è –ú–ï–¢–ê–î–ê–ù–ù–´–ï –ù–ï –î–û–ë–ê–í–ò–õ–ò–°–¨ –ö –¢–ï–ö–°–¢–£!")

        # –°–æ–∑–¥–∞–µ–º embeddings –∏–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        # print(f"\nüîÑ –°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è {len(texts_for_embedding)} —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤...")
        logger.info(f"–°–æ–∑–¥–∞–µ–º embeddings –¥–ª—è {len(texts_for_embedding)} —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤")

        embeddings = self.create_embeddings(texts_for_embedding)

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å
        start_id = self.index.ntotal
        self.index.add(embeddings)

        # print(f"‚úÖ Embeddings —Å–æ–∑–¥–∞–Ω—ã –∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ FAISS –∏–Ω–¥–µ–∫—Å")
        logger.info(f"Embeddings —Å–æ–∑–¥–∞–Ω—ã –∏ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ FAISS –∏–Ω–¥–µ–∫—Å")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –º–∞–ø–ø–∏–Ω–≥–∏ (–æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
        added_ids = []
        for i, chunk in enumerate(chunks):
            faiss_id = start_id + i
            added_ids.append(faiss_id)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.metadata[chunk.chunk_id] = {
                'text': chunk.text,  # ‚ùó –°–æ—Ö—Ä–∞–Ω—è–µ–º –ò–°–•–û–î–ù–´–ô —Ç–µ–∫—Å—Ç, –Ω–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π
                'source_file': chunk.source_file,
                'chunk_index': chunk.chunk_index,
                'metadata': chunk.metadata,
                'added_date': datetime.now().isoformat(),
                'faiss_id': faiss_id
            }

            # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
            self.id_to_chunk_id[faiss_id] = chunk.chunk_id
            self.chunk_id_to_id[chunk.chunk_id] = faiss_id

        final_message = f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö. –í—Å–µ–≥–æ –≤ –∏–Ω–¥–µ–∫—Å–µ: {self.index.ntotal}"
        # print(final_message)
        logger.info(final_message)

        return added_ids

    def _add_chunks_legacy(self, chunks: List[TextChunk]) -> List[int]:
        """–°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        if self.index is None:
            self.create_index()

        if not chunks:
            return []

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embeddings
        texts = [chunk.text for chunk in chunks]
        embeddings = self.create_embeddings(texts)

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å
        start_id = self.index.ntotal
        self.index.add(embeddings)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏ –º–∞–ø–ø–∏–Ω–≥–∏
        added_ids = []
        for i, chunk in enumerate(chunks):
            faiss_id = start_id + i
            added_ids.append(faiss_id)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.metadata[chunk.chunk_id] = {
                'text': chunk.text,
                'source_file': chunk.source_file,
                'chunk_index': chunk.chunk_index,
                'metadata': chunk.metadata,
                'added_date': datetime.now().isoformat(),
                'faiss_id': faiss_id,
                'has_visual_vector': False  # ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            }

            # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
            self.id_to_chunk_id[faiss_id] = chunk.chunk_id
            self.chunk_id_to_id[chunk.chunk_id] = faiss_id

        logger.info(f"–î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å. –í—Å–µ–≥–æ –≤ –∏–Ω–¥–µ–∫—Å–µ: {self.index.ntotal}")
        return added_ids

    # ‚úÖ –ù–û–í–´–ï –º–µ—Ç–æ–¥—ã –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞

    def add_text_chunk(self, chunk: TextChunk) -> int:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —á–∞–Ω–∫ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
        if not self.enable_visual_search:
            # Fallback –Ω–∞ —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É
            return self._add_chunks_legacy([chunk])[0]

        if self.text_index is None:
            self.create_index()

        # üîß –ò–°–ü–†–ê–í–õ–Ø–ï–ú: –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ —Ç–µ–∫—Å—Ç—É –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        metadata = chunk.metadata
        full_text_parts = [chunk.text]

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ —Ç–µ–∫—Å—Ç—É –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        if metadata.get('title'):
            full_text_parts.append(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {metadata['title']}")
        if metadata.get('description'):
            full_text_parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {metadata['description']}")
        if metadata.get('category'):
            full_text_parts.append(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {metadata['category']}")
        if metadata.get('parent'):
            full_text_parts.append(f"–†–∞–∑–¥–µ–ª: {metadata['parent']}")

        combined_text = " ".join(full_text_parts)

        # üîß DEBUG: –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –≤–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥)
        debug_msg = f"üîß –¢–ï–ö–°–¢-–¢–û–õ–¨–ö–û {chunk.source_file} (—á–∞–Ω–∫ {chunk.chunk_index}):"
        # print(debug_msg)
        logger.info(debug_msg)

        size_msg = f"   üìù –ò—Å—Ö–æ–¥–Ω—ã–π: {len(chunk.text)} —Å–∏–º–≤–æ–ª–æ–≤ ‚Üí –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π: {len(combined_text)} —Å–∏–º–≤–æ–ª–æ–≤"
        # print(size_msg)
        logger.info(size_msg)

        meta_msg = f"   üè∑Ô∏è title='{metadata.get('title', '–ù–ï–¢')}' | description='{metadata.get('description', '–ù–ï–¢')}' | category='{metadata.get('category', '–ù–ï–¢')}'"
        # print(meta_msg)
        logger.info(meta_msg)

        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π embedding –∏–∑ –†–ê–°–®–ò–†–ï–ù–ù–û–ì–û —Ç–µ–∫—Å—Ç–∞
        text_embedding = self.create_embeddings([combined_text])

        # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        text_faiss_id = self.text_index.ntotal
        self.text_index.add(text_embedding)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.metadata[chunk.chunk_id] = {
            'text': chunk.text,  # ‚ùó –°–æ—Ö—Ä–∞–Ω—è–µ–º –ò–°–•–û–î–ù–´–ô —Ç–µ–∫—Å—Ç, –Ω–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π
            'source_file': chunk.source_file,
            'chunk_index': chunk.chunk_index,
            'metadata': chunk.metadata,
            'added_date': datetime.now().isoformat(),
            'text_faiss_id': text_faiss_id,
            'visual_faiss_id': None,
            'has_visual_vector': False
        }

        # –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
        self.text_id_to_chunk_id[text_faiss_id] = chunk.chunk_id
        self.chunk_id_to_ids[chunk.chunk_id] = {'text_id': text_faiss_id, 'visual_id': None}

        success_msg = f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π —á–∞–Ω–∫ –¥–æ–±–∞–≤–ª–µ–Ω: —Ç–µ–∫—Å—Ç_id={text_faiss_id}"
        # print(success_msg)
        logger.info(success_msg)

        return text_faiss_id
    def add_multimodal_chunk(self, chunk: TextChunk, visual_vector: np.ndarray) -> Tuple[int, int]:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π —á–∞–Ω–∫ (—Ç–µ–∫—Å—Ç + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)

        Args:
            chunk: –¢–µ–∫—Å—Ç–æ–≤—ã–π —á–∞–Ω–∫ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            visual_vector: –í–∏–∑—É–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

        Returns:
            Tuple[int, int]: (text_faiss_id, visual_faiss_id)
        """
        if not self.enable_visual_search:
            raise ValueError("–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ —Ç—Ä–µ–±—É—é—Ç enable_visual_search=True")

        if self.text_index is None or self.visual_index is None:
            self.create_index()

        # 1. üîß –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —Ç–µ–∫—Å—Ç–æ–≤–∞—è —á–∞—Å—Ç—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        metadata = chunk.metadata
        full_text_parts = [chunk.text]

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞–∂–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ —Ç–µ–∫—Å—Ç—É –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        if metadata.get('title'):
            full_text_parts.append(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {metadata['title']}")
        if metadata.get('description'):
            full_text_parts.append(f"–û–ø–∏—Å–∞–Ω–∏–µ: {metadata['description']}")
        if metadata.get('category'):
            full_text_parts.append(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {metadata['category']}")
        if metadata.get('parent'):
            full_text_parts.append(f"–†–∞–∑–¥–µ–ª: {metadata['parent']}")

        combined_text = " ".join(full_text_parts)

        # üîß DEBUG: –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –≤–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–≤–æ–¥)
        debug_msg = f"üîß –ú–£–õ–¨–¢–ò–ú–û–î–ê–õ {chunk.source_file} (—á–∞–Ω–∫ {chunk.chunk_index}):"
        # print(debug_msg)
        logger.info(debug_msg)

        size_msg = f"   üìù –ò—Å—Ö–æ–¥–Ω—ã–π: {len(chunk.text)} —Å–∏–º–≤–æ–ª–æ–≤ ‚Üí –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π: {len(combined_text)} —Å–∏–º–≤–æ–ª–æ–≤"
        # print(size_msg)
        logger.info(size_msg)

        meta_msg = f"   üè∑Ô∏è title='{metadata.get('title', '–ù–ï–¢')}' | description='{metadata.get('description', '–ù–ï–¢')}' | category='{metadata.get('category', '–ù–ï–¢')}'"
        # print(meta_msg)
        logger.info(meta_msg)

        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –†–ê–°–®–ò–†–ï–ù–ù–´–ô —Ç–µ–∫—Å—Ç
        text_embedding = self.create_embeddings([combined_text])
        text_faiss_id = self.text_index.ntotal
        self.text_index.add(text_embedding)

        # 2. –î–æ–±–∞–≤–ª—è–µ–º –≤–∏–∑—É–∞–ª—å–Ω—É—é —á–∞—Å—Ç—å (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–∏–∑—É–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        if self.index_type == "FlatIP":
            normalized_visual = visual_vector / np.linalg.norm(visual_vector)
        else:
            normalized_visual = visual_vector

        visual_faiss_id = self.visual_index.ntotal
        self.visual_index.add(normalized_visual.reshape(1, -1))

        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.metadata[chunk.chunk_id] = {
            'text': chunk.text,  # ‚ùó –°–æ—Ö—Ä–∞–Ω—è–µ–º –ò–°–•–û–î–ù–´–ô —Ç–µ–∫—Å—Ç, –Ω–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π
            'source_file': chunk.source_file,
            'chunk_index': chunk.chunk_index,
            'metadata': chunk.metadata,
            'added_date': datetime.now().isoformat(),
            'text_faiss_id': text_faiss_id,
            'visual_faiss_id': visual_faiss_id,
            'has_visual_vector': True
        }

        # 4. –û–±–Ω–æ–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
        self.text_id_to_chunk_id[text_faiss_id] = chunk.chunk_id
        self.visual_id_to_chunk_id[visual_faiss_id] = chunk.chunk_id
        self.chunk_id_to_ids[chunk.chunk_id] = {
            'text_id': text_faiss_id,
            'visual_id': visual_faiss_id
        }

        success_msg = f"‚úÖ –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π —á–∞–Ω–∫ –¥–æ–±–∞–≤–ª–µ–Ω: —Ç–µ–∫—Å—Ç_id={text_faiss_id}, –≤–∏–∑—É–∞–ª_id={visual_faiss_id}"
        # print(success_msg)
        logger.info(success_msg)

        return text_faiss_id, visual_faiss_id
    def search(self, query: str, k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å–æ —Å—Ç–∞—Ä—ã–º API)"""
        if not self.enable_visual_search:
            # –°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞
            return self._search_legacy(query, k, score_threshold)
        else:
            # –í –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ –∏—â–µ–º –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –∏–Ω–¥–µ–∫—Å—É
            return self.search_text(query, k, score_threshold)

    def _search_legacy(self, query: str, k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:
        """–°—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        if self.index is None or self.index.ntotal == 0:
            logger.warning("–ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç –∏–ª–∏ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            return []

        # –°–æ–∑–¥–∞–µ–º embedding –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.create_embeddings([query])


        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        scores, indices = self.index.search(query_embedding, k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx == -1:  # FAISS –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç -1 –¥–ª—è –Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                continue

            if score < score_threshold:
                continue

            chunk_id = self.id_to_chunk_id.get(idx)
            if chunk_id and chunk_id in self.metadata:
                result = {
                    'chunk_id': chunk_id,
                    'score': float(score),
                    'text': self.metadata[chunk_id]['text'],
                    'source_file': self.metadata[chunk_id]['source_file'],
                    'metadata': self.metadata[chunk_id]['metadata']
                }
                results.append(result)

        return results

    # ‚úÖ –ù–û–í–´–ï –º–µ—Ç–æ–¥—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞

    def search_text(self, query: str, k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:
        """–¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –≤ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–º —Ä–µ–∂–∏–º–µ"""
        if not self.enable_visual_search or self.text_index is None or self.text_index.ntotal == 0:
            return []

        query_embedding = self.create_embeddings([query])
        scores, indices = self.text_index.search(query_embedding, k)

        return self._format_search_results(scores[0], indices[0], "text", score_threshold)

    def search_visual(self, visual_query: np.ndarray, k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:
        """–í–∏–∑—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫"""
        if not self.enable_visual_search or self.visual_index is None or self.visual_index.ntotal == 0:
            return []

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
        if self.index_type == "FlatIP":
            normalized_query = visual_query / np.linalg.norm(visual_query)
        else:
            normalized_query = visual_query

        scores, indices = self.visual_index.search(normalized_query.reshape(1, -1), k)

        return self._format_search_results(scores[0], indices[0], "visual", score_threshold)

    def search_multimodal(self, text_query: str = None, visual_query: np.ndarray = None,
                          k: int = 5, text_weight: float = 0.5) -> List[Dict[str, Any]]:
        """
        –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫

        Args:
            text_query: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            visual_query: –í–∏–∑—É–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            text_weight: –í–µ—Å —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞ (0.0-1.0)

        Returns:
            List[Dict]: –û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        if not self.enable_visual_search:
            # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
            return self.search_text(text_query, k) if text_query else []

        all_results = {}  # {chunk_id: {'text_score': X, 'visual_score': Y}}

        # –¢–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
        if text_query:
            text_results = self.search_text(text_query, k=k * 2)  # –ë–µ—Ä–µ–º –±–æ–ª—å—à–µ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
            for result in text_results:
                chunk_id = result['chunk_id']
                all_results[chunk_id] = all_results.get(chunk_id, {})
                all_results[chunk_id]['text_score'] = result['score']
                all_results[chunk_id]['result_data'] = result

        # –í–∏–∑—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
        if visual_query is not None:
            visual_results = self.search_visual(visual_query, k=k * 2)
            for result in visual_results:
                chunk_id = result['chunk_id']
                all_results[chunk_id] = all_results.get(chunk_id, {})
                all_results[chunk_id]['visual_score'] = result['score']
                if 'result_data' not in all_results[chunk_id]:
                    all_results[chunk_id]['result_data'] = result

        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
        combined_results = []
        for chunk_id, scores in all_results.items():
            text_score = scores.get('text_score', 0.0)
            visual_score = scores.get('visual_score', 0.0)

            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
            if 'text_score' in scores and 'visual_score' in scores:
                # –û–±–∞ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞—à–ª–∏ —ç—Ç–æ—Ç —á–∞–Ω–∫
                combined_score = text_weight * text_score + (1 - text_weight) * visual_score
                search_type = "multimodal"
            elif 'text_score' in scores:
                combined_score = text_score * text_weight
                search_type = "text_only"
            else:
                combined_score = visual_score * (1 - text_weight)
                search_type = "visual_only"

            result = scores['result_data'].copy()
            result['combined_score'] = combined_score
            result['search_type'] = search_type
            result['text_score'] = text_score
            result['visual_score'] = visual_score

            combined_results.append(result)

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É score
        combined_results.sort(key=lambda x: x['combined_score'], reverse=True)

        return combined_results[:k]

    def _format_search_results(self, scores: np.ndarray, indices: np.ndarray,
                               search_type: str, score_threshold: float) -> List[Dict[str, Any]]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞"""
        results = []

        if search_type == "text":
            id_mapping = self.text_id_to_chunk_id
        elif search_type == "visual":
            id_mapping = self.visual_id_to_chunk_id
        else:
            id_mapping = self.id_to_chunk_id  # Fallback

        for score, idx in zip(scores, indices):
            if idx == -1 or score < score_threshold:
                continue

            chunk_id = id_mapping.get(idx)
            if chunk_id and chunk_id in self.metadata:
                result = {
                    'chunk_id': chunk_id,
                    'score': float(score),
                    'search_type': search_type,
                    'text': self.metadata[chunk_id]['text'],
                    'source_file': self.metadata[chunk_id]['source_file'],
                    'metadata': self.metadata[chunk_id]['metadata']
                }
                results.append(result)

        return results

    # ‚úÖ –û–ë–ù–û–í–õ–ï–ù–ù–´–ï –º–µ—Ç–æ–¥—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏

    def save_index(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–∞ –¥–∏—Å–∫"""
        logger.info("–°–æ—Ö—Ä–∞–Ω—è–µ–º FAISS –∏–Ω–¥–µ–∫—Å(—ã) –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")

        if self.enable_visual_search:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
            if self.text_index is not None:
                faiss.write_index(self.text_index, str(self.text_index_path))
                logger.info(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {self.text_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")

            if self.visual_index is not None:
                faiss.write_index(self.visual_index, str(self.visual_index_path))
                logger.info(f"‚úÖ –í–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {self.visual_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ–¥–∏–Ω—ã–π –∏–Ω–¥–µ–∫—Å
            if self.index is not None:
                faiss.write_index(self.index, str(self.index_path))
                logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        with open(self.metadata_path, 'wb') as f:
            pickle.dump(self.metadata, f)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
        if self.enable_visual_search:
            mappings_data = {
                'text_id_to_chunk_id': {str(k): v for k, v in self.text_id_to_chunk_id.items()},
                'visual_id_to_chunk_id': {str(k): v for k, v in self.visual_id_to_chunk_id.items()},
                'chunk_id_to_ids': self.chunk_id_to_ids
            }
        else:
            mappings_data = {
                'id_to_chunk_id': {str(k): v for k, v in self.id_to_chunk_id.items()},
                'chunk_id_to_id': self.chunk_id_to_id
            }

        with open(self.mappings_path, 'w', encoding='utf-8') as f:
            json.dump(mappings_data, f, ensure_ascii=False, indent=2)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        config_data = {
            'model_name': self.model_name,
            'index_type': self.index_type,
            'dimension': self.dimension,
            'enable_visual_search': self.enable_visual_search,  # ‚úÖ –ù–û–í–û–ï
            'text_dimension': self.text_dimension,  # ‚úÖ –ù–û–í–û–ï
            'visual_dimension': self.visual_dimension,  # ‚úÖ –ù–û–í–û–ï
            'total_vectors': self._get_total_vectors(),
            'text_vectors': self.text_index.ntotal if self.text_index else 0,  # ‚úÖ –ù–û–í–û–ï
            'visual_vectors': self.visual_index.ntotal if self.visual_index else 0,  # ‚úÖ –ù–û–í–û–ï
            'last_updated': datetime.now().isoformat()
        }
        with open(self.config_path, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, ensure_ascii=False, indent=2)

    def _get_total_vectors(self) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        if self.enable_visual_search:
            text_count = self.text_index.ntotal if self.text_index else 0
            visual_count = self.visual_index.ntotal if self.visual_index else 0
            return text_count + visual_count
        else:
            return self.index.ntotal if self.index else 0

    def load_index(self) -> bool:


        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å –¥–∏—Å–∫–∞"""
        required_paths = [self.metadata_path, self.mappings_path, self.config_path]

        missing_files = [str(p) for p in required_paths if not p.exists()]
        if missing_files:
            logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã –∏–Ω–¥–µ–∫—Å–∞: {', '.join(missing_files)}")
            return False

        try:
            logger.info("–ó–∞–≥—Ä—É–∂–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å(—ã) –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            self.model_name = config['model_name']
            self.index_type = config['index_type']
            self.dimension = config['dimension']
            # ‚úÖ –ù–û–í–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            self.enable_visual_search = config.get('enable_visual_search', False)
            self.text_dimension = config.get('text_dimension', self.dimension)
            self.visual_dimension = config.get('visual_dimension', settings.VISUAL_EMBEDDING_DIMENSION)

            if self.enable_visual_search:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
                if self.text_index_path.exists():
                    self.text_index = faiss.read_index(str(self.text_index_path))
                    logger.info(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {self.text_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")

                if self.visual_index_path.exists():
                    self.visual_index = faiss.read_index(str(self.visual_index_path))
                    logger.info(f"‚úÖ –í–∏–∑—É–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {self.visual_index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
            else:
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –µ–¥–∏–Ω—ã–π –∏–Ω–¥–µ–∫—Å
                if self.index_path.exists():
                    self.index = faiss.read_index(str(self.index_path))
                    logger.info(f"‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(self.metadata_path, 'rb') as f:
                self.metadata = pickle.load(f)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
            with open(self.mappings_path, 'r', encoding='utf-8') as f:
                mappings_data = json.load(f)

            if self.enable_visual_search:
                # –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
                self.text_id_to_chunk_id = {int(k): v for k, v in mappings_data.get('text_id_to_chunk_id', {}).items()}
                self.visual_id_to_chunk_id = {int(k): v for k, v in
                                              mappings_data.get('visual_id_to_chunk_id', {}).items()}
                self.chunk_id_to_ids = mappings_data.get('chunk_id_to_ids', {})
            else:
                # –°—Ç–∞—Ä—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
                self.id_to_chunk_id = {int(k): v for k, v in mappings_data.get('id_to_chunk_id', {}).items()}
                self.chunk_id_to_id = mappings_data.get('chunk_id_to_id', {})

            logger.info(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {len(self.metadata)} —á–∞–Ω–∫–æ–≤")
            return True

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            return False

    def get_index_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–Ω–¥–µ–∫—Å–∞"""
        if self.enable_visual_search:
            return self._get_multimodal_statistics()
        else:
            return self._get_legacy_statistics()

    def _get_legacy_statistics(self) -> Dict[str, Any]:
        """–°—Ç–∞—Ä–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        if self.index is None:
            return {'status': 'not_initialized'}

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        sources_stats = {}
        for metadata in self.metadata.values():
            source = metadata['source_file']
            if source not in sources_stats:
                sources_stats[source] = 0
            sources_stats[source] += 1

        return {
            'status': 'ready',
            'total_vectors': self.index.ntotal,
            'dimension': self.dimension,
            'index_type': self.index_type,
            'model_name': self.model_name,
            'total_chunks': len(self.metadata),
            'sources_count': len(sources_stats),
            'sources_distribution': sources_stats,
            'is_trained': getattr(self.index, 'is_trained', True),
            'enable_visual_search': False
        }

    def _get_multimodal_statistics(self) -> Dict[str, Any]:
        """–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        stats = {
            'status': 'ready' if (self.text_index is not None or self.visual_index is not None) else 'not_initialized',
            'client_id': self.client_id,
            'enable_visual_search': True,
            'text_dimension': self.text_dimension,
            'visual_dimension': self.visual_dimension,
            'text_vectors_count': self.text_index.ntotal if self.text_index else 0,
            'visual_vectors_count': self.visual_index.ntotal if self.visual_index else 0,
            'total_chunks': len(self.metadata),
            'multimodal_chunks': sum(1 for chunk in self.metadata.values()
                                     if chunk.get('has_visual_vector', False))
        }

        if stats['status'] == 'ready':
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            sources_stats = {}
            categories = {}
            file_types = {}
            visual_content_count = 0

            for chunk_data in self.metadata.values():
                metadata = chunk_data.get('metadata', {})

                # –ò—Å—Ç–æ—á–Ω–∏–∫–∏
                source = chunk_data.get('source_file', 'unknown')
                sources_stats[source] = sources_stats.get(source, 0) + 1

                # –¢–∏–ø—ã —Ñ–∞–π–ª–æ–≤
                file_type = metadata.get('file_type', 'unknown')
                file_types[file_type] = file_types.get(file_type, 0) + 1

                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏
                category = metadata.get('category', 'uncategorized')
                categories[category] = categories.get(category, 0) + 1

                # –í–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                if chunk_data.get('has_visual_vector', False):
                    visual_content_count += 1

            stats.update({
                'sources_count': len(sources_stats),
                'sources_distribution': sources_stats,
                'file_types_distribution': file_types,
                'categories_distribution': categories,
                'visual_content_ratio': visual_content_count / len(self.metadata) if self.metadata else 0
            })

        return stats

    # –ú–µ—Ç–æ–¥—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    def get_index_stats(self) -> Dict[str, Any]:
        """–ê–ª–∏–∞—Å –¥–ª—è get_index_statistics (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)"""
        return self.get_index_statistics()

    def get_all_chunks(self) -> List[Dict[str, Any]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏"""
        return list(self.metadata.values())

    def get_chunks_by_source(self, source_file: str) -> List[Dict[str, Any]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ —á–∞–Ω–∫–∏ –∏–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
        return [metadata for metadata in self.metadata.values()
                if metadata['source_file'] == source_file]

    def remove_chunks(self, chunk_ids: List[str]) -> bool:
        """–£–¥–∞–ª—è–µ—Ç —á–∞–Ω–∫–∏ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        removed_count = 0
        for chunk_id in chunk_ids:
            if chunk_id in self.metadata:
                # –£–¥–∞–ª—è–µ–º –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                del self.metadata[chunk_id]
                removed_count += 1

                # –£–¥–∞–ª—è–µ–º –∏–∑ –º–∞–ø–ø–∏–Ω–≥–æ–≤
                if self.enable_visual_search:
                    # –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
                    if chunk_id in self.chunk_id_to_ids:
                        ids_info = self.chunk_id_to_ids[chunk_id]

                        if ids_info.get('text_id') is not None:
                            self.text_id_to_chunk_id.pop(ids_info['text_id'], None)

                        if ids_info.get('visual_id') is not None:
                            self.visual_id_to_chunk_id.pop(ids_info['visual_id'], None)

                        del self.chunk_id_to_ids[chunk_id]
                else:
                    # –°—Ç–∞—Ä—ã–µ –º–∞–ø–ø–∏–Ω–≥–∏
                    if chunk_id in self.chunk_id_to_id:
                        faiss_id = self.chunk_id_to_id[chunk_id]
                        self.id_to_chunk_id.pop(faiss_id, None)
                        del self.chunk_id_to_id[chunk_id]

        if removed_count > 0:
            logger.warning(
                f"–£–¥–∞–ª–µ–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è {removed_count} —á–∞–Ω–∫–æ–≤. –î–ª—è –ø–æ–ª–Ω–æ–π –æ—á–∏—Å—Ç–∫–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞")

        return removed_count > 0

    def clear_index(self):
        """–û—á–∏—â–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ –≤—Å–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
        logger.warning(f"–û—á–∏—â–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ –∫–ª–∏–µ–Ω—Ç–∞ {self.client_id}")

        # –û—á–∏—â–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
        self.index = None
        self.text_index = None
        self.visual_index = None

        # –û—á–∏—â–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        self.metadata = {}
        self.id_to_chunk_id = {}
        self.chunk_id_to_id = {}
        self.text_id_to_chunk_id = {}
        self.visual_id_to_chunk_id = {}
        self.chunk_id_to_ids = {}

        # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã —Å –¥–∏—Å–∫–∞
        for path in [self.index_path, self.text_index_path, self.visual_index_path,
                     self.metadata_path, self.mappings_path, self.config_path]:
            if path.exists():
                path.unlink()

    # ‚úÖ –ù–û–í–´–ï –º–µ—Ç–æ–¥—ã –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏

    def get_visual_vector(self, chunk_id: str) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤–∏–∑—É–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —á–∞–Ω–∫–∞"""
        if not self.enable_visual_search or chunk_id not in self.metadata:
            return None

        chunk_data = self.metadata[chunk_id]
        visual_faiss_id = chunk_data.get('visual_faiss_id')

        if visual_faiss_id is not None and self.visual_index is not None:
            try:
                return self.visual_index.reconstruct(visual_faiss_id)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –≤–∏–∑—É–∞–ª—å–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è {chunk_id}: {e}")

        return None

    def get_similar_visual_chunks(self, chunk_id: str, k: int = 5) -> List[Dict[str, Any]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –≤–∏–∑—É–∞–ª—å–Ω–æ –ø–æ—Ö–æ–∂–∏–µ —á–∞–Ω–∫–∏ –Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π"""
        visual_vector = self.get_visual_vector(chunk_id)
        if visual_vector is None:
            return []

        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ (–∏—Å–∫–ª—é—á–∞–µ–º —Å–∞–º —á–∞–Ω–∫)
        results = self.search_visual(visual_vector, k=k + 1)

        # –£–±–∏—Ä–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —á–∞–Ω–∫ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        filtered_results = []
        for result in results:
            if result.get('chunk_id') != chunk_id:
                filtered_results.append(result)

        return filtered_results[:k]

    def export_visual_vectors(self) -> Dict[str, Any]:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã"""
        if not self.enable_visual_search or self.visual_index is None:
            return {'error': '–í–∏–∑—É–∞–ª—å–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã'}

        export_data = {
            'client_id': self.client_id,
            'visual_vectors_count': self.visual_index.ntotal,
            'visual_dimension': self.visual_dimension,
            'vectors': [],
            'metadata': []
        }

        for chunk_data in self.metadata.values():
            if chunk_data.get('has_visual_vector', False):
                visual_id = chunk_data.get('visual_faiss_id')
                if visual_id is not None:
                    try:
                        vector = self.visual_index.reconstruct(visual_id)
                        export_data['vectors'].append(vector.tolist())
                        export_data['metadata'].append({
                            'chunk_id': chunk_data.get('chunk_id'),
                            'source_file': chunk_data.get('source_file'),
                            'visual_faiss_id': visual_id
                        })
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –≤–µ–∫—Ç–æ—Ä–∞ {visual_id}: {e}")

        return export_data


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –∏ –º–∏–≥—Ä–∞—Ü–∏–∏
def check_index_compatibility(client_id: str) -> Dict[str, Any]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
    client_dir = settings.CLIENTS_DIR / client_id

    compatibility = {
        'client_id': client_id,
        'has_legacy_index': (client_dir / "index.faiss").exists(),
        'has_multimodal_index': (client_dir / "text_index.faiss").exists() or (
                    client_dir / "visual_index.faiss").exists(),
        'config_exists': (client_dir / "config.json").exists(),
        'migration_needed': False,
        'recommendations': []
    }

    if compatibility['has_legacy_index'] and not compatibility['has_multimodal_index']:
        compatibility['migration_needed'] = True
        compatibility['recommendations'].append("–ú–æ–∂–Ω–æ –º–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É")

    if compatibility['has_multimodal_index']:
        compatibility['recommendations'].append("–ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")

    if not compatibility['config_exists']:
        compatibility['recommendations'].append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")

    return compatibility


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ–∂–∏–º–∞
def create_faiss_manager(client_id: str, enable_visual_search: bool = None) -> FAISSManager:
    """
    –°–æ–∑–¥–∞–µ—Ç FAISS –º–µ–Ω–µ–¥–∂–µ—Ä —Å –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ–∂–∏–º–∞

    Args:
        client_id: ID –∫–ª–∏–µ–Ω—Ç–∞
        enable_visual_search: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –≤–∏–∑—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫.
                             None = –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

    Returns:
        FAISSManager: –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä
    """
    # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞
    if enable_visual_search is None:
        client_dir = settings.CLIENTS_DIR / client_id
        config_path = client_dir / "config.json"

        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                enable_visual_search = config.get('enable_visual_search', False)
                logger.info(f"–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∂–∏–º–∞ –¥–ª—è {client_id}: –≤–∏–∑—É–∞–ª—å–Ω—ã–π_–ø–æ–∏—Å–∫={enable_visual_search}")
            except Exception:
                enable_visual_search = settings.ENABLE_VISUAL_SEARCH_BY_DEFAULT
        else:
            enable_visual_search = settings.ENABLE_VISUAL_SEARCH_BY_DEFAULT

    return FAISSManager(client_id=client_id, enable_visual_search=enable_visual_search)