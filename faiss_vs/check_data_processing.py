#!/usr/bin/env python3
"""
–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –ü–ï–†–ï–î –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π
–°–æ—Ö—Ä–∞–Ω–∏ –∫–∞–∫: faiss_vs/check_data_processing.py
"""

import json
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent / "src"))

from src.data.loaders import DocumentLoader
from src.data.chunkers import DocumentChunker


def check_json_data_quality(client_id: str):
    """
    üìä –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å—Ö–æ–¥–Ω—ã—Ö JSON –¥–∞–Ω–Ω—ã—Ö
    """
    print("üìä –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê –ò–°–•–û–î–ù–´–• JSON –î–ê–ù–ù–´–•")
    print("=" * 50)

    # –ò—â–µ–º JSON —Ñ–∞–π–ª –∫–ª–∏–µ–Ω—Ç–∞
    from src.config import settings
    client_dir = settings.FAISS_INDEX_DIR / "clients" / client_id
    json_file = client_dir / "data.json"

    if not json_file.exists():
        print(f"‚ùå JSON —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {json_file}")
        return None

    print(f"üìÑ –ó–∞–≥—Ä—É–∂–∞–µ–º: {json_file}")

    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if 'result' not in data:
        print("‚ùå –ù–µ—Ç –ø–æ–ª—è 'result' –≤ JSON")
        return None

    documents = data['result']
    print(f"üìà –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    stats = {
        'total': len(documents),
        'with_description': 0,
        'empty_description': 0,
        'with_parent': 0,
        'empty_parent': 0,
        'pdf_files': 0,
        'other_files': 0
    }

    problematic_docs = []
    good_examples = []

    print(f"\nüìã –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:")

    for i, doc in enumerate(documents):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
        description = doc.get('Description', '')
        if description and description.strip():
            stats['with_description'] += 1
            if len(good_examples) < 3:
                good_examples.append({
                    'index': i,
                    'description': description,
                    'parent': doc.get('Parent', ''),
                    'url': doc.get('ID', '')
                })
        else:
            stats['empty_description'] += 1
            problematic_docs.append({
                'index': i,
                'problem': '–ü—É—Å—Ç–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ',
                'url': doc.get('ID', ''),
                'parent': doc.get('Parent', '')
            })

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        parent = doc.get('Parent', '')
        if parent and parent.strip():
            stats['with_parent'] += 1
        else:
            stats['empty_parent'] += 1

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
        url = doc.get('ID', '')
        if url.lower().endswith('.pdf'):
            stats['pdf_files'] += 1
        else:
            stats['other_files'] += 1

    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(
        f"   üìù –° –æ–ø–∏—Å–∞–Ω–∏–µ–º: {stats['with_description']}/{stats['total']} ({stats['with_description'] / stats['total'] * 100:.1f}%)")
    print(
        f"   üìÅ –° –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π: {stats['with_parent']}/{stats['total']} ({stats['with_parent'] / stats['total'] * 100:.1f}%)")
    print(f"   üìÑ PDF —Ñ–∞–π–ª–æ–≤: {stats['pdf_files']}")
    print(f"   üìé –î—Ä—É–≥–∏—Ö —Ñ–∞–π–ª–æ–≤: {stats['other_files']}")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ö–æ—Ä–æ—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã
    if good_examples:
        print(f"\n‚úÖ –•–û–†–û–®–ò–ï –ü–†–ò–ú–ï–†–´ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:")
        for example in good_examples:
            print(f"   {example['index']}. {example['description']}")
            print(f"      üìÅ {example['parent']}")
            print(f"      üîó {example['url'][:60]}...")

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–±–ª–µ–º—ã
    if problematic_docs:
        print(f"\n‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´:")
        for problem in problematic_docs[:5]:  # –ü–µ—Ä–≤—ã–µ 5
            print(f"   {problem['index']}. {problem['problem']}")
            print(f"      üìÅ {problem['parent']}")
            print(f"      üîó {problem['url'][:60]}...")

    return documents


def test_loader_processing(documents_sample):
    """
    üîß –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ DocumentLoader
    """
    print(f"\nüîß –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ë–†–ê–ë–û–¢–ö–ò –ß–ï–†–ï–ó DocumentLoader")
    print("=" * 55)

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π JSON —Ñ–∞–π–ª
    test_data = {
        'result': documents_sample[:3]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –¥–æ–∫—É–º–µ–Ω—Ç–∞
    }

    test_file = Path("test_documents.json")
    with open(test_file, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False, indent=2)

    print(f"üìÑ –°–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª: {test_file}")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º DocumentLoader
    try:
        loader = DocumentLoader()

        print(f"üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ DocumentLoader...")

        # –ù–ï —Å–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã, —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        documents = loader.load_from_json(str(test_file))

        print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")

        # –°–∏–º—É–ª–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        for i, doc_info in enumerate(documents):
            print(f"\nüìã –î–æ–∫—É–º–µ–Ω—Ç {i}:")
            print(f"   üîó URL: {doc_info.get('ID', '–ù–ï–¢')}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫ –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {}

            # –ö–æ–ø–∏—Ä—É–µ–º –≤—Å–µ –ø–æ–ª—è (–∫–∞–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ)
            for key, value in doc_info.items():
                if key not in ['url', 'filename', 'ID']:
                    normalized_key = key.lower()
                    metadata[normalized_key] = value

            # –î–æ–±–∞–≤–ª—è–µ–º –º–∞–ø–ø–∏–Ω–≥–∏
            if 'description' in metadata and metadata['description']:
                metadata['title'] = metadata['description']

            if 'parent' in metadata and metadata['parent']:
                metadata['category'] = metadata['parent']

            print(f"   üìù –ò—Å—Ö–æ–¥–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ: {doc_info.get('Description', '–ù–ï–¢')}")
            print(f"   üìù –§–∏–Ω–∞–ª—å–Ω—ã–π title: {metadata.get('title', '–ù–ï–¢')}")
            print(f"   üìÅ –ò—Å—Ö–æ–¥–Ω—ã–π Parent: {doc_info.get('Parent', '–ù–ï–¢')}")
            print(f"   üìÅ –§–∏–Ω–∞–ª—å–Ω–∞—è category: {metadata.get('category', '–ù–ï–¢')}")
            print(f"   üóÇÔ∏è –í—Å–µ–≥–æ –ø–æ–ª–µ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(metadata)}")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–±–ª–µ–º—ã
            if not metadata.get('title'):
                print(f"   ‚ùå –ü–†–û–ë–õ–ï–ú–ê: –ù–µ—Ç title!")
            if not metadata.get('category'):
                print(f"   ‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: –ù–µ—Ç category!")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è loader: {e}")
    finally:
        # –£–¥–∞–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
        if test_file.exists():
            test_file.unlink()


def test_chunker_enhancement(sample_text: str, metadata: dict):
    """
    ‚úÇÔ∏è –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
    """
    print(f"\n‚úÇÔ∏è –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –û–ë–û–ì–ê–©–ï–ù–ò–Ø –ß–ê–ù–ö–û–í")
    print("=" * 40)

    chunker = DocumentChunker()

    print(f"üìù –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç ({len(sample_text)} —Å–∏–º–≤–æ–ª–æ–≤):")
    print(f"   {sample_text[:150]}...")

    print(f"\nüè∑Ô∏è –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:")
    for key, value in metadata.items():
        print(f"   {key}: {value}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –º–µ—Ç–æ–¥ –æ–±–æ–≥–∞—â–µ–Ω–∏—è
    if hasattr(chunker, 'create_enhanced_chunk_text'):
        print(f"\n‚úÖ –ú–µ—Ç–æ–¥ create_enhanced_chunk_text –Ω–∞–π–¥–µ–Ω!")

        try:
            enhanced_text = chunker.create_enhanced_chunk_text(sample_text, metadata)

            print(f"üöÄ –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ({len(enhanced_text)} —Å–∏–º–≤–æ–ª–æ–≤):")
            print(f"   {enhanced_text[:300]}...")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords_found = []
            if metadata.get('title') and metadata['title'] in enhanced_text:
                keywords_found.append('title')
            if metadata.get('category') and metadata['category'] in enhanced_text:
                keywords_found.append('category')

            print(f"‚úÖ –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ: {keywords_found}")

            if len(keywords_found) > 0:
                print(f"‚úÖ –û–ë–û–ì–ê–©–ï–ù–ò–ï –†–ê–ë–û–¢–ê–ï–¢!")
                return True
            else:
                print(f"‚ùå –û–ë–û–ì–ê–©–ï–ù–ò–ï –ù–ï –†–ê–ë–û–¢–ê–ï–¢!")
                return False

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è: {e}")
            return False
    else:
        print(f"‚ùå –ú–µ—Ç–æ–¥ create_enhanced_chunk_text –ù–ï –ù–ê–ô–î–ï–ù!")
        print(f"üí° –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –≤ chunkers.py")
        return False


def recommend_fixes(documents):
    """
    üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—é
    """
    print(f"\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ")
    print("=" * 40)

    # –°—á–∏—Ç–∞–µ–º –ø—Ä–æ–±–ª–µ–º—ã
    empty_descriptions = sum(1 for doc in documents
                             if not doc.get('Description', '').strip())
    empty_parents = sum(1 for doc in documents
                        if not doc.get('Parent', '').strip())

    print(f"üìä –ù–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º:")
    print(f"   üìù –ü—É—Å—Ç—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π: {empty_descriptions}/{len(documents)}")
    print(f"   üìÅ –ü—É—Å—Ç—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {empty_parents}/{len(documents)}")

    if empty_descriptions > len(documents) * 0.3:
        print(f"\n‚ùå –ö–†–ò–¢–ò–ß–ù–û: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—É—Å—Ç—ã—Ö –æ–ø–∏—Å–∞–Ω–∏–π!")
        print(f"   üõ†Ô∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:")
        print(f"   1. –ü—Ä–æ–≤–µ—Ä—å –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö –≤ 1–°")
        print(f"   2. –î–æ–±–∞–≤—å fallback –≤ loaders.py:")
        print(f"      if not metadata.get('title'):")
        print(f"          metadata['title'] = filename  # –ò—Å–ø–æ–ª—å–∑—É–π –∏–º—è —Ñ–∞–π–ª–∞")

    if empty_parents > len(documents) * 0.2:
        print(f"\n‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–ê: –ú–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏!")
        print(f"   üõ†Ô∏è –î–æ–±–∞–≤—å –≤ loaders.py:")
        print(f"      metadata['category'] = metadata.get('category') or '–ü—Ä–æ—á–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã'")

    print(f"\nüîß –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:")
    print(f"   1. ‚úÖ –ò—Å–ø—Ä–∞–≤–∏—Ç—å loaders.py - –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –º–∞–ø–ø–∏–Ω–≥ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö")
    print(f"   2. ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –≤ chunkers.py - –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞")
    print(f"   3. ‚úÖ –ò—Å–ø—Ä–∞–≤–∏—Ç—å faiss_manager.py - —É–º–Ω—ã–π –ø–æ–∏—Å–∫")
    print(f"   4. ‚úÖ –¢–æ–ª—å–∫–æ –ø–æ—Ç–æ–º –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å!")


def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python check_data_processing.py <client_id>")
        sys.exit(1)

    client_id = sys.argv[1]

    try:
        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ JSON –¥–∞–Ω–Ω—ã–µ
        documents = check_json_data_quality(client_id)

        if not documents:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
            return

        # 2. –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ loader
        test_loader_processing(documents)

        # 3. –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±–æ–≥–∞—â–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
        sample_metadata = {
            'title': '–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–∫–æ–Ω –∏ –¥–≤–µ—Ä–µ–π',
            'description': '–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –æ–∫–æ–Ω –∏ –¥–≤–µ—Ä–µ–π',
            'category': '–§–∞–π–ª—ã –ø–æ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ',
            'filename': 'spec_okna_dveri.pdf'
        }

        sample_text = "–î–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—é –æ–∫–æ–Ω–Ω—ã—Ö –∏ –¥–≤–µ—Ä–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞."

        enhancement_works = test_chunker_enhancement(sample_text, sample_metadata)

        # 4. –î–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommend_fixes(documents)

        print(f"\nüéØ –§–ò–ù–ê–õ–¨–ù–´–ï –í–´–í–û–î–´:")
        print(f"   üìä –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: ‚úÖ")
        print(f"   üîß –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {'‚úÖ' if enhancement_works else '‚ùå'}")

        if enhancement_works:
            print(f"\n‚úÖ –ú–û–ñ–ù–û –ü–ï–†–ï–ò–ù–î–ï–ö–°–ò–†–û–í–ê–¢–¨!")
            print(f"   python quick_cleanup.py {client_id}")
            print(f"   # –ü–æ—Ç–æ–º —Å–æ–∑–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å –∑–∞–Ω–æ–≤–æ")
        else:
            print(f"\n‚ùå –°–ù–ê–ß–ê–õ–ê –ò–°–ü–†–ê–í–¨ –ö–û–î, –ø–æ—Ç–æ–º –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∏—Ä—É–π!")
            print(f"   1. –î–æ–±–∞–≤—å –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –≤ chunkers.py")
            print(f"   2. –ò—Å–ø—Ä–∞–≤—å –ø–æ–∏—Å–∫ –≤ faiss_manager.py")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()