#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ FAISS –∏–Ω–¥–µ–∫—Å–µ
"""

import sys
import json
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent / "src"))

from src.document_processor import DocumentProcessor


def check_metadata(client_id: str):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –∏–Ω–¥–µ–∫—Å–µ –∫–ª–∏–µ–Ω—Ç–∞"""
    print(f"üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞: {client_id}")
    print("=" * 60)

    processor = DocumentProcessor(client_id=client_id)

    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–Ω–¥–µ–∫—Å–∞
    stats = processor.get_index_statistics()

    if stats.get('status') != 'ready':
        print(f"‚ùå –ò–Ω–¥–µ–∫—Å –Ω–µ –≥–æ—Ç–æ–≤: {stats}")
        return

    print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–¥–µ–∫—Å–∞:")
    print(f"   –í–µ–∫—Ç–æ—Ä–æ–≤: {stats.get('total_vectors', 0)}")
    print(f"   –ß–∞–Ω–∫–æ–≤: {stats.get('total_chunks', 0)}")
    print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {stats.get('sources_count', 0)}")

    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
    all_chunks = processor.faiss_manager.get_all_chunks()

    if not all_chunks:
        print("‚ùå –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ")
        return

    print(f"\nüìã –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ({len(all_chunks)} —á–∞–Ω–∫–æ–≤):")

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata_fields = set()
    missing_fields = []
    field_stats = {}

    for i, chunk in enumerate(all_chunks):
        chunk_metadata = chunk.get('metadata', {})

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        for field in chunk_metadata.keys():
            metadata_fields.add(field)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—è–º
        for field, value in chunk_metadata.items():
            if field not in field_stats:
                field_stats[field] = {'count': 0, 'examples': []}
            field_stats[field]['count'] += 1
            if len(field_stats[field]['examples']) < 3:
                field_stats[field]['examples'].append(str(value)[:100])

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤ –ø–æ–¥—Ä–æ–±–Ω–æ
        if i < 3:
            print(f"\nüìÑ –ß–∞–Ω–∫ {i}:")
            print(f"   –ò—Å—Ç–æ—á–Ω–∏–∫: {chunk.get('source_file', 'unknown')}")
            print(f"   –¢–µ–∫—Å—Ç: {chunk.get('text', '')[:100]}...")
            print(f"   –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(chunk_metadata)}")

            print("   üìù –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ:")
            for key, value in chunk_metadata.items():
                value_str = str(value)[:50]
                if len(str(value)) > 50:
                    value_str += "..."
                print(f"      {key}: {value_str}")

    print(f"\nüìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö:")
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π: {len(metadata_fields)}")
    print(f"   –ü–æ–ª—è: {sorted(metadata_fields)}")

    print(f"\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—è–º:")
    for field, stats in sorted(field_stats.items()):
        coverage = (stats['count'] / len(all_chunks)) * 100
        print(f"   {field}:")
        print(f"      –ü–æ–∫—Ä—ã—Ç–∏–µ: {stats['count']}/{len(all_chunks)} ({coverage:.1f}%)")
        print(f"      –ü—Ä–∏–º–µ—Ä—ã: {stats['examples']}")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–∂–Ω—ã–µ –ø–æ–ª—è
    important_fields = ['title', 'description', 'category', 'parent', 'date', 'source_url', 'file_type']

    print(f"\n‚ö†Ô∏è  –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–∂–Ω—ã—Ö –ø–æ–ª–µ–π:")
    for field in important_fields:
        if field in metadata_fields:
            coverage = (field_stats[field]['count'] / len(all_chunks)) * 100
            status = "‚úÖ" if coverage > 80 else "‚ö†Ô∏è" if coverage > 50 else "‚ùå"
            print(f"   {status} {field}: {coverage:.1f}% –ø–æ–∫—Ä—ã—Ç–∏–µ")
        else:
            print(f"   ‚ùå {field}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

    # –ò—â–µ–º –ø—Ä–æ–±–ª–µ–º—ã
    print(f"\nüö® –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:")

    # –ß–∞–Ω–∫–∏ –±–µ–∑ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    chunks_without_source = [c for c in all_chunks if not c.get('metadata', {}).get('source_url')]
    if chunks_without_source:
        print(f"   - {len(chunks_without_source)} —á–∞–Ω–∫–æ–≤ –±–µ–∑ source_url")

    chunks_without_category = [c for c in all_chunks if not c.get('metadata', {}).get('category')]
    if chunks_without_category:
        print(f"   - {len(chunks_without_category)} —á–∞–Ω–∫–æ–≤ –±–µ–∑ category")

    chunks_without_title = [c for c in all_chunks if not c.get('metadata', {}).get('title')]
    if chunks_without_title:
        print(f"   - {len(chunks_without_title)} —á–∞–Ω–∫–æ–≤ –±–µ–∑ title")

    return {
        'total_chunks': len(all_chunks),
        'metadata_fields': sorted(metadata_fields),
        'field_stats': field_stats
    }


def export_metadata_sample(client_id: str, output_file: str = None):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –æ–±—Ä–∞–∑–µ—Ü –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
    processor = DocumentProcessor(client_id=client_id)
    all_chunks = processor.faiss_manager.get_all_chunks()

    if not all_chunks:
        print("‚ùå –ù–µ—Ç —á–∞–Ω–∫–æ–≤ –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞")
        return

    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 —á–∞–Ω–∫–æ–≤ –∫–∞–∫ –æ–±—Ä–∞–∑–µ—Ü
    sample_chunks = all_chunks[:5]

    sample_data = {
        'client_id': client_id,
        'total_chunks': len(all_chunks),
        'sample_count': len(sample_chunks),
        'chunks': []
    }

    for chunk in sample_chunks:
        sample_data['chunks'].append({
            'source_file': chunk.get('source_file'),
            'text_preview': chunk.get('text', '')[:200],
            'metadata': chunk.get('metadata', {})
        })

    if not output_file:
        output_file = f"metadata_sample_{client_id}.json"

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –û–±—Ä–∞–∑–µ—Ü –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_file}")


def main():
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  python check_metadata.py <client_id>")
        print("  python check_metadata.py <client_id> export")
        sys.exit(1)

    client_id = sys.argv[1]
    command = sys.argv[2] if len(sys.argv) > 2 else "check"

    try:
        if command == "export":
            export_metadata_sample(client_id)
        else:
            result = check_metadata(client_id)

            if result:
                print(f"\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
                print(f"   –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {result['total_chunks']}")
                print(f"   –ü–æ–ª–µ–π –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {len(result['metadata_fields'])}")

                # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —ç–∫—Å–ø–æ—Ä—Ç
                export_sample = input(f"\n–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–±—Ä–∞–∑–µ—Ü –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö? (y/N): ")
                if export_sample.lower() == 'y':
                    export_metadata_sample(client_id)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()